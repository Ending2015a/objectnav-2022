
_import_:
  - "/src/configs/envs/kemono_train.yaml"

habitat_config: "/src/configs/test/test_hm3d.val_mini.rgbd.yaml"

goal_mapping:
  0: chair
  1: bed
  2: plant
  3: toilet
  4: tv_monitor
  5: sofa

env_id: HabitatEnv-v4
agent_type: sac2
log_path: "/src/logs/kemono_agent/${agent_type}/${env_id}"
seq_len: 16
batch_size: 64
num_workers: 256
group_size: 128

envs:
  env_id: ${env_id}
  # imported from envs/kemono_train.yaml
  planner:
    planner_name: random
    planner_kwargs:
      max_distance: 4.0
      min_distance: 2.0
      replan_duration: 30
      replan_distance: 1.0
    enable_reward: true

  monitor:
    root_dir: "${log_path}monitor/"
    video: true
    video_kwargs:
      interval: null # cubic
      fps: 10

  trajectory_recorder:
    interval: 1

  vec_envs:
    n_envs: 2
    gpus: [0, 1]

# === Network rchitecutre ===

backbone:
  net_config:
    depth:
      type: nature_cnn
      mlp_units: [512]
    small_map:
      type: nature_cnn
      mlp_units: [512]
    plan_distance:
      type: bin_embed
      min_val: 0.1
      max_val: 8.0
      num_embed: 72
      embed_dim: 32
      use_log_scale: true
    plan_angle:
      type: bin_embed
      min_val: -3.1415926
      max_val: 3.1415926
      num_embed: 72
      embed_dim: 32
      use_log_scale: false
  fuse_config:
    type: mlp
    mlp_units: [512]

agent:
  type: ${agent_type}
  runner:
    goal_mapping: ${goal_mapping}
    report_n_episodes: 100
  policy:
    backbone: ${backbone}
    hidden_units: 512
    mlp_units: [256] # 512 -> 256 -> n_actions
    skip_conn: false
    rnn_type: lstm
  value:
    backbone: ${backbone}
    hidden_units: 512
    mlp_units: [256] # 512 -> 256 -> n_actions
    skip_conn: false
    rnn_type: lstm
  batch_size: ${batch_size}
  value_estimate: softq # sac or softq
  learn_alpha: false # disable alpha learning
  init_alpha: 0.5
  entropy_scale: 0.2
  policy_lr: 3.0e-4
  value_lr: 3.0e-4
  alpha_lr: 3.0e-4
  loss_type: huber
  huber_delta: 0.01
  tau: 0.005
  gamma: 0.98
  warmup_steps: 10000
  n_steps: 32
  n_gradsteps: 32
  train_dataset: ${train_dataset}

# === Dataset ===

train_dataset:
  # multiprocess:
  #   num_threads: 8
  #   env_vars: {}
  # prefetch: ${macro.group_size}
  stream_producer:
    # expert_stream:
    #   producer:
    #     data_root: "/src/logs/kemono_expert/${macro.env_id}/train/trajs/"
    #     glob_paths: "*.trajectory.npz"
    #     shuffle: true
    #   manager:
    #     max_to_keep: 2000
    #     clean_unused: false
    #   drawer:
    #     max_to_draw: 500
    #   zero_pad:
    #     chunk_size: ${macro.seq_len}
    policy_stream:
      producer:
        data_root: "${envs.monitor.root_dir}trajs/"
        glob_paths: "*.trajectory.npz"
        shuffle: true
      manager:
        max_to_keep: 2000
        clean_unused: true
      drawer:
        max_to_draw: 500
      zero_pad:
        chunk_size: ${seq_len}
  dataset:
    num_workers: ${num_workers}
    group_size: ${group_size}
    seq_len: ${seq_len}
    drop_remainder: false
  sampler:
    num_workers: ${num_workers}
    batch_size: ${batch_size}
    time_first: true
    drop_remainder: false

# === Training ===

checkpoint:
  monitor: "train/avg_success"
  save_last: True
  save_top_k: 40
  mode: max
  every_n_epochs: 100
  verbose: True

trainer:
  max_epochs: 10000000
  accelerator: gpu
  devices: [2]
  default_root_dir: ${log_path}
  sync_batchnorm: True

ddp:
  find_unused_parameters: true
